
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>OEM Few-Shot Challenge</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

    <!-- <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
    <link rel="icon" type="image/png" href="assets/img/OpenEarthMap_Logo_v3.png"> 
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="./assets/css/app.css">

    <link rel="stylesheet" href="./assets/css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="./assets/js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-10 text-center col-md-offset-1">
                <img src="./assets/img/OpenEarthMap_Logo_v2.png" alt="OpenEarthMap" height="100">
            </h2>
            <p class="col-md-10 text-center col-md-offset-1">
                <image src="./assets/img/img2.webp" class="img-responsive" alt="overview"></image>
            </p>
        </div>
        <div class="row">
            <h2 class="col-md-10 text-center col-md-offset-1">
                OpenEarthMap Land Cover Mapping Few-Shot Challenge</br> 
                <small>
                    Co-organized with the <a href="https://sites.google.com/view/l3divu2024/overview" target="_blank">L3D-IVU CVPR 2024</a> Workshop
                </small>
            </h2>
            <div class="col-md-10 text-center col-md-offset-1">
                <ul class="list-inline">
                    <li>
                        <a href="https://codalab.lisn.upsaclay.fr/competitions/17568" target="_blank">
                          Codalab Submission Portal
                        </a>
                    </li>
                    <li>
                        <a href="https://zenodo.org/records/10828417" target="_blank">
                          Dataset Download
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/cliffbb/OEM-Fewshot-Challenge" target="_blank">
                          Baseline Code
                        </a>
                    </li>
                    <li>
                        <a href="#rules">
                            Challenge Rules 
                        </a>
                    </li>
                    <li>
                        <a href="#paper">
                            Challenge Papers Submission
                        </a>
                    </li>
                </ul>   
            </div>
        </div>
        <br>

        <div class="row">
            <h4 class="col-md-10 col-md-offset-1">
                <b style="color: brown;"><strike>Challenge evaluation phase (final round) is opened!</strike></b></br>
                <p>
                    <strike>Participants who have submitted a paper should download the 
                    <a href="https://zenodo.org/records/10828417" target="_blank">`testset`</a>
                    for the final evaluation submission.</strike>
                </p></br>
                <p>
                    <strike>Participants should take note of the final submission deadline: <b>March 29, 2024 11:59 pm 
                    Pacific Time</b> (<span style="color: brown;">strict, no extension</span>).</strike>
                </p></br>
                <b style="color: brown;"><strike>Submission of challenge papers is <a href="#submit">opened.</a></strike></b></br>
                <b style="color: brown;"><strike>Challenge development phase (1st round) is opened!</strike></b>
            </h4>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    About the Challenge
                </h3>
                <p class="text-justify">
                    The <a href="https://open-earth-map.org/" target="_blank">OpenEarthMap</a> is a remote sensing (RS) image semantic segmentation 
                    benchmark dataset consisting of aerial/satellite images covering 97 regions from 44 countries across 6 continents at a spatial resolution
                    of 0.25&ndash;0.5m ground sampling distance for global high-resolution land cover mapping. It presents an advancement in geographical 
                    diversity and annotation quality, enabling models to generalize worldwide. This challenge extends the original RS semantic 
                    segmentation task of the OpenEarthMap benchmark to generalized few-shot semantic segmentation (GFSS) task in RS image understanding.</br>
                
                    The challenge aims to evaluate and benchmark learning methods for few-shot semantic segmentation on the OpenEarthMap dataset
                    to promote research in AI for social good. The motivation is to enable researchers to develop few-shot learning algorithms 
                    for high-resolution RS image semantic segmentation, which is a fundamental problem in various applications of RS image understanding,
                    such as disaster response, urban planning, and natural resource management.</br>
                
                    The challenge is part of the <a href="https://sites.google.com/view/l3divu2024/overview" target="_blank">3rd Workshop on Learning with Limited 
                    Labelled Data for Image and Video Understanding (L3D-IVU)</a> in conjunction with CVPR 2024 Conference. The scientific papers of the best submissions
                    will be presented in oral at the 3rd L3D-IVU Workshop @ CVPR 2024 Conference, and will also be published in the CVPR 2024 Workshops Proceedings. 
                </p> 
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Competition Phases
                </h3>
                <p class="text-justify">
                    <b>Phase 1 (Development Phase):</b> Participants are provided with the OpenEarthMap Few-Shot dataset for training and validation to train and 
                    validate their algorithms (<i>See the <a href="#dataset">dataset section</a> for details of the training and validation sets</i>).
                    Participants can submit results on the validation set to the <a href="https://codalab.lisn.upsaclay.fr/competitions/17568" target="_blank">Codalab competition submission portal</a> 
                    to get feedback on the performance. The performance of the best submission from each account will be displayed on the leaderboard. 
                    In parallel, participants have to submit a <a href="#paper">challenge paper</a> of their proposed method to be eligible to enter Phase 2.</br>
                    
                    <b>Phase 2 (Evaluation Phase):</b> This is the final phase. Participants receive the testset of the OpenEarthMap Few-Shot dataset 
                    (<i>See the <a href="#dataset">dataset section</a> for details of the testset</i>). Participants submit their results on 
                    the testset to the <a href="https://codalab.lisn.upsaclay.fr/competitions/17568" target="_blank">Codalab competition submission portal</a> within <b>six days</b> from the release of the testing set.
                    After evaluation of the results, the top winners are announced.</br>

                    <strong>Note that the top winners of this challenge are determined not only on the performance on the leaderboard, but in addition to the novelty 
                    of their proposed method as described in the manuscripts they submit in the Phase 1. 
                    The manuscripts of the top winners will be included in the CVPR 2024 Workshops Proceedings.</strong>
                </p>
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Important Dates
                </h3>
                <p class="text-justify" style="color: brown;">
                    All the deadlines are strict, no extension will be given (<strong>11:59 pm Pacific Time</strong>).
                </p>
                <p class="text-justify">
                    <ul>
                        <li>
                            <b>Feb 05, 2024:</b> Opening challenge 1st round (development phase).
                        </li>
                        <li>
                            <b>Mar 01, 2024:</b> Opening paper submission.
                        </li>
                        <li>
                            <b>Mar 22, 2024:</b> Paper submission deadline.
                        </li>
                        <li>
                            <b>Mar 23, 2024:</b> Opening challenge 2nd round (evaluation phase).
                        </li>
                        <li>
                            <b>Mar 29, 2024:</b> Challenge submission deadline.
                        </li>
                        <li>
                            <b>Apr 07, 2024:</b> Paper notification to authors.
                        </li>
                        <li>
                            <b>Apr 14, 2024:</b> Camera-ready deadline.
                        </li>
                        <li>
                            <b>Apr 21, 2024:</b> Challenge winner announcement.
                        </li>
                    </ul>
                </p>
                <p class="text-justify">
                    The L3D-IVU Workshop is on Tuesday, June 18, 2024.
                </p>
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3 id="task">
                    Task & Metrics
                </h3>
                <p class="text-justify">
                    The main task of the OpenEarthMap Few-Shot Challenge is a <strong>5-shot</strong> multi-class semantic segmentation, i.e., generalized few-shot
                    semantic segmentation (GFSS) task, with <strong>4 novel classes</strong> and <strong>7 base classes</strong>. 
                    Given a <i>support set</i> of images with their labels, participants are to predict segmentation maps of all images in a given <i>query set</i>
                    (Note, the labels for the images in the <i>query set</i> are withheld and are used for evaluation).</br>
                    
                    The <i>support set</i> consists of 20 image-label pair examples, 5-set examples for each of the 4 novel classes. 
                    The labels for each image in the <i>support set</i> do not contain any of the base classes. Also, in each 5-set examples, the labels contain 
                    only one novel class (i.e., one novel class per 5-set examples). However, the labels for the images in the <i>query set</i> contain 
                    both the base classes and the novel classes. Below are examples of novel classes in the <i>support_set</i> (first two columns), 
                    and base classes + novel classes in the <i>query_set</i> (last two columns).</br>
                    <image src="./assets/img/fewshot-examples1.png" class="img-responsive" alt="few-shot dataset"></image></br>

                    In this challenge, the GFSS task is in two phases: the <strong>development phase</strong>  and the <strong>evaluation phase</strong>.
                    In the development phase, the participants will be given training data for pre-training their backbone networks. The participants will use
                    the training data and a <i>support set</i> of 20 image-label pair examples to predict the segmentation maps of 30 images in a 
                    given <i>query set</i>, which contains both base classes and novel classes. Here, the participants submit their results to get performance feedback.
                    <strong>Note: participants need to submit their challenge paper to be eligible to enter the evaluation phase</strong>.</br>

                    In the evaluation phase, the participants will use the training data and a different <i>support set</i> of 20 image-label pair examples
                    to predict the segmentation maps of 80 images in a given <i>query set</i>, which also contains both base classes and novel classes.                 
                    Note that both the <i>support set</i> and the <i>query set</i> for the development phase are different from the ones for the evaluation phase. 
                    However, in terms of class definition, the base classes are the same, but the novel classes are different. Here, the participants submit their
                    results for final evaluation. The performance of the best submission from each submission account will be displayed on the leaderboard. 
                    See the <a href="#rules">challenge rules</a> for the submission format.
                </p>
                <p class="text-justify">
                    For evaluation, we resort to the <strong>mean intersection-over-union (mIoU)</strong>, which is the average value over the IoUs of all the 
                    target classes. Note that the mIoU is computed over the target classes only without the background. Following the GFSS literature, 
                    we will use the following three metrics:<br>
                    <ul>
                        <li>
                            <b>base mIoU:</b> The average value over the IoUs of all the target base classes in the <i>query set</i>.
                        </li>
                        <li>
                            <b>novel mIoU:</b> The average value over the IoUs of all the target novel classes in the <i>query set</i>.
                        </li>
                        <li>
                            <b>average base-novel mIoU:</b> The average of the   <b>base mIoU</b> and the  <b>novel mIoU</b>
                        </li>
                    </ul>
                </p> 
                <p class="text-justify">
                    However, in this challenge, more weight is placed on the <strong>novel mIoU</strong>. Thus, instead of using the <b>average base-novel mIoU</b>
                    score for the performance evaluation and ranking, we resort to a <i>weighted-sum</i> of <b>base mIoU</b> and <b>novel mIoU</b>, which is 
                    computed using 0.4:0.6 weights for the base classes and the novel classes, respectively (0.4*<b>base mIoU</b> + 0.6*<b>novel mIoU</b>). 
                    The weights are derived from the state-of-the-art results presented in the
                    <a href="https://arxiv.org/pdf/2211.14126.pdf" target="_blank">GFSS baseline</a> adopted in this challenge.                    
                    Note that the top winners are determined not only on the performance on the leaderboard but in addition to the novelty of 
                    their proposed method described in their submitted manuscripts.
                </p>               
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3 id="rules">
                    Rules of the Game
                </h3>
                <p class="text-justify">
                    The participants are supposed to submit a single compressed `.zip` file which contains the predicted segmentation maps of all images
                    in a given <i>query set</i> at the <a href="https://codalab.lisn.upsaclay.fr/competitions/17568" target="_blank">Codalab 
                    competition submission portal</a>. The predicted segmentation maps are supposed to be in `xxxx.png` format. The `xxxx` is the name 
                    of the image in the <i>query set</i>. For example, given a query image `accra_2.tif`, the predicted segmentation map should be
                    named as `accra_2.png`. Any other format of submission will not be accepted.
                </p>
                <p class="text-justify" style="margin-bottom:0px;">
                    To ensure fair evaluation and reproducible results, the following rules are set for this challenge competition as well:
                    <ul>
                        <li>
                            Submissions must not use any <b>remote sensing</b> dataset apart from the one distributed for this challenge. 
                        </li>
                        <li>
                            All submissions are also required to submit a challenge paper describing their proposed methods.
                        </li>
                        <li>
                            Each team can have only one submission account. 
                        </li>
                        <li>
                            A team can not have more than five members. A member can only be part of one team.
                        </li>
                        <li>
                            Each team can make only 10 submissions per day in the development phase. 
                        </li>
                        <li>
                            Each team can make a total of 10 submissions in the evaluation phase. 
                        </li>
                        <li>
                            The best submission of a team in the evaluation phase is used to rank the team, and it is selected for the final evaluation.
                        </li>
                        <li>
                            The top winners are determined by both the performance on the leaderboard and the novelty of their proposed method.
                        </li>
                        <li>
                            The top winners will be asked to submit their codes for cross-checking their results.
                        </li>                       
                        <li>
                            The top winners' submissions must exceed the baseline score of the challenge to be eligible for the challenge prize.
                        </li>
                        <li>
                            The organizers of the challenge are not allowed to participate.
                        </li>
                    </ul>
                </p>
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3 id="dataset">
                    The Dataset
                </h3>
                <p class="text-justify">
                    The OpenEarthMap few-shot learning challenge dataset consists of only 408 samples of the original
                    <a href="https://open-earth-map.org/" target="_blank">OpenEarthMap</a> benchmark dataset for RS image semantic segmentation.
                    The challenge dataset extends the original 8 semantic classes of the OpenEarthmap benchmark to 15 classes, which is  
                    split into 7:4:4 for <i>train_base_class</i>, <i>val_novel_class</i>, and <i>test_novel_class</i> disjointed sets, respectively
                    (i.e., <i>train_base_class</i> &cap; <i>val_novel_class</i> &cap; <i>test_novel_class</i> &equals; &empty;). 
                    See the <a href="#task">challenge task</a> for the purpose of each class split and the number of classes in each class split.</br>

                    The 408 samples are also split into 258 as trainset, 50 as valset, and 100 as testset. The trainset is for pre-training a
                    backbone network. It contains only the images and labels of the <i>train_base_class</i> split. Both the valset and the testset
                    consist of a <i>support set</i> and a <i>query set</i>. The valset and the testset contain the images and labels
                    of the <i>val_novel_class</i> and the <i>test_novel_class</i> splits, respectively.  A detailed description of the 
                    challenge dataset can be found <a href="https://zenodo.org/records/10828417" target="_blank">here</a>, 
                    where the dataset can also be downloaded. The README in the <a href="https://github.com/cliffbb/OEM-Fewshot-Challenge" target="_blank">
                    baseline code</a> also explains how the challenge dataset can be used with the baseline model.
                </p>
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Baseline Model 
                </h3>
                <p class="text-justify">
                    A GFSS framework, called <a href="https://arxiv.org/pdf/2211.14126.pdf" target="_blank">distilled information maximization (DIaM)</a>, 
                    with a PSPNet architecture of EfficientNet-B4 encoder from the PyTorch segmentation models library is presented 
                    as a baseline model. The baseline code can be used as a starter code for the challenge submission. To run it, follow the
                    README instructions presented <a href="https://github.com/cliffbb/OEM-Fewshot-Challenge" target="_blank">here</a>.

                    After running the code, an output folder ``results`` which contains ``preds`` and ``targets`` folders of the model's segmentation 
                    prediction maps and the corresponding targets, respectively, are created. Based on the rules mentioned above, only the ``preds`` folder 
                    which contains the predicted segmentation maps in `.png` file format is required for the submission. Please feel free to contact 
                    the challenge organizers for any question regarding the baseline code.
                </p>
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Awards &amp; Prizes
                </h3>
                <p class="text-justify">
                    Three (3) teams will be declared as winners: the 1st, 2nd, and 3rd ranked teams.</br>

                    The winning teams will have the opportunity to present their papers in oral at the 3rd L3D-IVU Workshop @ CVPR 2024 Conference.</br>

                    The papers of the winning teams will be published in the CVPR2 024 Workshops Proceedings. 
                    Note that for a paper to be included in the proceedings, it should be <strong>full-length (5&ndash;8 pages, excluding 
                    references) paper and not published at CVPR 2024</strong>.</br>

                    The authors of the winning teams will be awarded certificates on the day of the 3rd L3D-IVU Workshop @ CVPR 2024 Conference.</br>

                    The 1st winning team will also receive a prize of <b>1000 USD</b>.
                </p>
                <hr>
            </div>
        </div>      

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3 id="paper">
                    Challenge Paper Submission
                </h3>
                <p class="text-justify">
                    As part of the <strong>challenge evaluation</strong>, the participants are required to submit a 5&ndash;8 pages (excluding references)
                    paper in parallel with the challenge submissions during the 1st round. The challenge winners are determined both by the performance 
                    on the leaderboard and the novelty of the proposed method as detailed in the submitted manuscripts. Each manuscript describes the 
                    challenge addressed problem (<i>i.e., generalized few-shot semantic segmentation task in RS image understanding</i>), 
                    the proposed method, and the experimental results.</br>

                    The papers will be peer-reviewed under single-blind policy. The accepted papers of the top winners will be published in the CVPR
                    2024 Workshops Proceedings. Note that the paper has to be <strong>full-length (5&ndash;8 pages, excluding references) and not 
                    published at CVPR 2024</strong>. The accepted papers will also be presented in oral at the 3rd L3D-IVU Workshop @ CVPR 2024 Conference.
                </p>
                <p class="text-justify" style="margin-bottom:0px;">
                    Submission guidelines: 
                    <ul>
                        <li>
                            Manuscripts are limited to 8-page maximum (excluding references).
                        </li>
                        <li>
                            Manuscripts must conform to a single-blind review policy.
                        </li>
                        <li>
                            Manuscripts should follow the CVPR 2024 paper style. Download a modified version from 
                            <a href="https://drive.google.com/file/d/1tK3AN8eX9WnFkmmvyE-4nKmUn06tbWKX/view?usp=sharing" target="_blank">here</a>
                            for the purpose of this challenge.
                        </li>
                        <li>
                            Authors must add their `Codalab account` which is used for the final phase of the challenge submissions. 
                        </li>
                        <li>
                            Supplementary material is not allowed. 
                        </li>
                        <li>
                            Submitted manuscripts will be rejected without review if they: <i>do not have accompanied challenge submissions</i>;
                            <i>exceed the page limit and/or not in the CVPR 2024 paper style provided</i>; and <i>violate the single-blind policy</i>.
                        </li>
                    </ul>
                </p>
                <p class="text-justify" style="margin-bottom:0px;" id="submit">
                    The key dates of the challenge paper submission are as follows:
                    <ul>
                        <li>
                            Opening paper submission: 1st March 2024.
                        </li>
                        <li>
                            Paper submission deadline: 22nd March 2024, 11:59 pm Pacific Time.
                        </li>
                        <li>
                            Notification to authors: 7th April 2024.
                        </li>
                        <li>
                            Camera-ready deadline: 14th April 2024, 11:59 pm Pacific Time.
                        </li>
                    </ul>
                </p>
                <p class="text-justify">
                    Manuscripts must be submitted online via the CMT submission system.
                    To submit, select "ChallengeL3DIVUCVPR24" under "Create new submission" menu 
                    at <a href="https://cmt3.research.microsoft.com/L3DIVUCVPR2024/" target="_blank">https://cmt3.research.microsoft.com/L3DIVUCVPR2024/</a>.
                </p>
                <hr>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Organizers
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            Clifford Broni-Bediako, RIKEN-AIP, Japan (<a href="mailto:clifford.broni-bediako@riken.jp">clifford.broni-bediako@riken.jp</a>)
                        </li>
                        <li>
                            Junshi Xia, RIKEN-AIP, Japan (<a href="mailto:junshi.xia@riken.jp">junshi.xia@riken.jp</a>)
                        </li>
                        <li>
                            Jian Song, The Unversity of Tokyo, Japan (<a href="song@ms.k.u-tokyo.ac.jp">song@ms.k.u-tokyo.ac.jp</a>)
                        </li>
                        <li>
                            Hongruixuan Chen, The Unversity of Tokyo, Japan (<a href="mailto:qschrx@g.ecc.u-tokyo.ac.jp">qschrx@g.ecc.u-tokyo.ac.jp</a>)
                        </li>
                        <li>
                            Naoto Yokaya, The Unversity of Tokyo/RIKEN-AIP, Japan (<a href="mailto:yokoya@k.u-tokyo.ac.jp">yokoya@k.u-tokyo.ac.jp</a>)
                        </li>
                    </ul>
                </p>
                <hr>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3 id="winners">
                    Winners/Accepted Papers
                </h3>
                <p class="text-justify">
                    <strong>The top-3 winners would be announced at L3D-IVU CVPR2024 Workshop on 18 June 2024. Accepted papers have been communicated to the authors</strong>
                </p>
                <h3 class="text-justify"> &nbsp; </h3>
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Citation
                </h3>
                <p class="text-justify">For any scientific publication using this data, the following paper should be cited:</p>
               
                <!-- <div class="col-md-10 col-md-offset-1 text-justify"> -->
                    <!-- <textarea id="bibtex" class="form-control" readonly> -->
<pre>
@misc{bronibediako2024GFSS,
      title={Generalized Few-Shot Semantic Segmentation in Remote Sensing: Challenge and Benchmark}, 
      author={Clifford Broni-Bediako and Junshi Xia and Jian Song and Hongruixuan Chen and Mennatullah Siam and Naoto Yokoya},
      year={2024},
      note={arXiv:2409.11227},
      url={https://arxiv.org/abs/2409.11227}, 
}
<!-- @InProceedings{Xia_2023_WACV,
    author    = {Xia, Junshi and Yokoya, Naoto and Adriano, Bruno and Broni-Bediako, Clifford},
    title     = {OpenEarthMap: A Benchmark Dataset for Global High-Resolution Land Cover Mapping},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2023},
    pages     = {6254-6264}
} -->
</pre>
                    <!-- </textarea> -->
                <!-- </div> -->
                <hr><hr>
            </div>
        </div>

    </div>
</body>
</html>
